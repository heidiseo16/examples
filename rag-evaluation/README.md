# Evaluate RAG with Ragas

This repository contains the codes and VESSL Run templates needed to load the [FEVER](https://huggingface.co/datasets/fever/fever) dataset into a vector database, execute the Retrieval Augmented Generation (RAG) chain with [Langchain](https://github.com/langchain-ai/langchain) to generate answers, and evaluate those answers with [Ragas](https://github.com/explodinggradients/ragas).

**FEVER (Fact Extraction and VERification)** consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as **Supported**, **Refuted** or **NotEnoughInfo**.

**Ragas** is a framework that helps you evaluate your RAG pipelines. Ragas provides you with the tools based on the latest research for evaluating LLM-generated text to give you insights about your RAG pipeline.

For further details, please refer to our blog post.

## Running Locally
> While you can run this pipeline in a local environment, we strongly recommend using VESSL Run for consistent environment management.
1. Clone the repository:
    ```sh
    $ git clone https://github.com/vessl-ai/examples.git
    $ cd examples/rag-evaluation
    ```

2. Install required dependencies:
    ```sh
    $ pip install -r requirements.txt
    ```

3. Set environment variables. You can replace each value with the value you want:
    ```sh
    $ export CHROMA_PATH="/chroma"
    $ export CHROMA_COLLECTION="fever-wiki-pages"
    $ export EMBEDDING_MODEL="BAAI/bge-m3"
    $ export DATA_PATH="/data"
    $ export LLM_ENDPOINT="https://api.openai.com"
    $ export LLM_MODEL="gpt-4o"
    $ export OPENAI_API_KEY="sk-1234567890"
    $ export RAG_PATTERN="naive"  # [naive, hyde, reranking, hyde-reranking]
    $ export RERANKER_MODEL="BAAI/bge-reranker-v2-m3"
    $ export OUTPUT_PATH="/output"
    $ export EVALUATION_ENDPOINT="https://api.openai.com"
    $ export EVALUATION_MODEL="gpt-4o"
    ```

4. Run the scripts consecutively:
    ```sh
    $ python 1-data-ingestion.py
    $ python 2-rag-chain.py
    $ python 3-rag-evaluation.py
    ```

5. (Optional) You can also evaluate LLMs without RAG:
    ```sh
    $ python 1-data-ingestion.py
    $ python 2.1-raw-llm-chain.py
    $ python 3.1-raw-llm-evaluation.py
    ```

## Running with VESSL Run
VESSL is a platform for deploying and managing AI applications. It allows you to deploy your AI applications on the cloud with a single command, and provides a web interface for managing your applications.

To run the RAG evaluation pipeline with VESSL Run, follow the steps below:

1. Install VESSL CLI and configure your identity:
    ```sh
    $ pip install --upgrade vessl
    $ vessl configure
    ```
2. (Optional) Create a new project (replace `${PROJECT_NAME}` with the project name):
    ```sh
    $ vessl project create ${PROJECT_NAME}
    $ vessl configure -p ${PROJECT_NAME}
    ```

3. (Optional) If you plan to use OpenAI API, create a secret with your OpenAI API key. Please refer to the [documentation](https://docs.vessl.ai/guides/organization/secrets) for instructions.

4. Create VESSL Datasets for persistent Chroma client, questions based on FEVER dataset, and RAG results respectively:
    ```sh
    $ vessl dataset create ${CHROMA_DATASET_NAME}
    $ vessl dataset create ${FEVER_DATASET_NAME}
    $ vessl dataset create ${RAG_RESULTS_DATASET_NAME}
    ```

5. Clone the repository:
    ```sh
    $ git clone https://github.com/vessl-ai/examples.git
    $ cd examples/rag-evaluation
    ```

6. You can change the environent variables in the YAML files, such as LLM endpoint, LLM model, embedding model and reranker model.

    1. You can choose from the following RAG types: `naive`, `hyde`, `reranking`, and `hyde-reranking`.
        ```yaml
        env:
            RAG_PATTERN: naive
        ```

    2. If you plan to use OpenAI API, you MUST make `OPENAI_API_KEY` refer to the secret you created above:
        ``` yaml
        env:
            OPENAI_API_KEY:
                secret: ${secret_name}
        ```

7. Create VESSL Run consecutively:
    ```sh
    $ vessl create run -f 1-data-ingestion.yaml
    $ vessl create run -f 2-rag-chain.yaml
    $ vessl create run -f 3-rag-evaluation.yaml
    ```

8. (Optional) You can also evaluate LLMs without RAG:
    ```sh
    $ vessl create run -f 1-data-ingestion.yaml
    $ vessl create run -f 2.1-raw-llm-chain.yaml
    $ vessl create run -f 3.1-raw-llm-evaluation.yaml
    ```

For additional information and support, please refer to the [VESSL documentation](https://docs.vessl.ai).

## Citation
```bibtex
@inproceedings{Thorne18Fever,
    author = {Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
    title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VERification}},
    booktitle = {NAACL-HLT},
    year = {2018}
}
```