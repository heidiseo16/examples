# Packages to install to enable faster inference (quantized models, FlashAttention2, etc)
autoawq==0.2.4
vllm==0.4.0.post1
flash-attn==2.5.7
