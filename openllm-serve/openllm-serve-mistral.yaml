# vessl serve revision create --serving openllm -f openllm-serve-vllm.yaml

message: OpenLLM mistralai/Mistral-7B-Instruct-v0.2 on vLLM
image: quay.io/vessl-ai/torch:2.2.0-cuda12.3-r3
resources:
  name: gpu-a10g-small
run: pip install --upgrade openllm[vllm]; openllm start mistralai/Mistral-7B-Instruct-v0.2 --backend vllm --port 3000 --max-model-len 4096
autoscaling:
  min: 1
  max: 3
  metric: gpu
  target: 0.8 # 80%
ports:
  - port: 3000
    name: openllm
    type: http
