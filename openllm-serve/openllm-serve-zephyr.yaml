# vessl serve revision create --serving openllm -f openllm-serve-zephyr.yaml

message: OpenLLM HuggingFaceH4/zephyr-7b-beta on vLLM
image: quay.io/vessl-ai/torch:2.2.0-cuda12.3-r3
resources:
  name: gpu-a10g-small
run: pip install --upgrade openllm[vllm]; openllm start HuggingFaceH4/zephyr-7b-beta --backend vllm --port 3000 --max-model-len 4096
autoscaling:
  min: 1
  max: 3
  metric: gpu
  target: 1
ports:
  - port: 3000
    name: openllm
    type: http
