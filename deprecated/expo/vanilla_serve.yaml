name: llama-3-tgi
message: llama 3 service on TGI
resources:
  cluster: vessl-gcp-oregon
  preset: gpu-l4-small-spot
image: ghcr.io/huggingface/text-generation-inference:1.4.5
run:
  - |-
    pip install autoawq
    text-generation-launcher --model-id $MODEL_NAME --quantize awq --max-input-length 3584 --max-total-tokens 4096 --port 8000
ports:
  - 8000
env:
  USE_FLASH_ATTENTION: "false"
  MODEL_NAME: casperhansen/llama-3-8b-instruct-awq
  HUGGING_FACE_HUB_TOKEN:
    secret: HF_TOKEN
service:
  autoscaling:
    max: 1
    metric: cpu
    min: 1
    target: 50
  expose: 8000
  monitoring:
    - port: 8000
      path: /metrics
